- General account Experience
    - Dollarama was a relatively happy customer that we had a vBlock with VMax in
    - Dollarama is a Retailer of the discount store variety
    - Their first experience with solutions based on Dell Servers and Dell Networking
    - Ultimately we have slowed down their deployment to the point where their executives are not only aware but are annoyed about it and we are blocking the business from executing on its plans.
        - 
    - There was even a 'last chance or you come pick up your stuff ' moment
- Short Solution Abstract
    - SE - No longer at the company but a good architect. One of the Graybreads @ VCE Canada
    - VCF on VxRail solution
        - 4 x E560 for the Management - Order Number 624085874 - CAD $284K
        - 5 x P580N (with Optane Cache Drives. One of the most souped up configs I have seen) - Order Number 624259008 - CAD $2.02M
        - 2 x S5232F 100GbE switches - 624085874 - CAD $60k
        - 1 x S3048 Mgmt Switch - 624085874 - CAD $8k
    - Workload
        - Initially Intended as a release valve for their current vBlock/VMax
        - Migrate a few hundred VMs over to a VxRail with the intention being, the best performing configuration 
            - General Purpose VMs is the target
        - There is also a conversation about SAP Hana. The intention being once they needed that they would purchase other VxRail clusters to create a more fine-tuned Architecture based on workload (P580s for SAP and other systems for General purpose)
        - VCF Picked, from what I understand, for 'Cloud Like' ops and integration with the VMware solution set
- Issues hit
    - They seem to have hit every 'not so bad but annoying' and a few 'well, that is actually bad' speed bump we have in the VxRail issues Bingo. 

And to further the issue, the customer was the one who reported most of them, not support finding them
        - Hardware Issues
            - P580 Node 5 was shipped with the wrong rails because DSA had the wrong sku. 
                - I gather it has had it for a while because another team we were helping at the time game to us with a wrong Rails problem
                - Have since brought this to Product Management (Ash)
                - Resolution:
                    - Project Manager arranged to ship the right rails
            - One of the P580 nodes was either shipped damaged or was damaged during Racking
                - Outside our control other than the face the CE could not have missed the damage based on this description so a bit of a mystery why it was reported by the customer
                - Resolution:
                    - We have a Mech Replacement out for Node 5 (This is a 400k post discount node)
            - P580 Node 2 had to have every part of the IO path to the drives swapped out
                - PCIe errors reported
                - Upon replacement we noticed the Backplane had bent pins
                - Resolution:
                    - Replacing multiple parts in Node 2
            - P580 Node 1 was also having diskissues traced to a cache drive
                - Resolution:
                    - Cache drive sent to engineering and replacement on the way
            - 
        - Software bugs
            - ESXi bugs
                - Admission control errors
                    - Apparently, all VxRail nodes have a silent error happening all the time in 7.0.100 and 7.0.101
                    - Checked in our labs and I see the same error
                    - Hosts eventually disconnect from vCenter
                    - Workaround
                        - There is a file on the hosts that can be changed but changing it requires that we turn of secure boot
                        - No Resolution. Awaiting a ESXi bug fix.
                        - This affects multiple VCF on VxRail customers since the bug is prevalent, however this is the first time I have directly seen the issue and the problems it caused 
                - Log Collection bug [Red Hot]
                    - When we attempt to collect logs it causes a latency spike which stuns VMs on the system. 
                    - This would be an issue for production as it drives towards supportability
                - A couple of other cosmetic issues that altogether make the solution look pretty rickety
                - On going bugs
        - VxRail Bugs
            - PTAgent issues
                - No details but the PTAgent rears it head again
            - VxRail first run drive claim failed
                - VxRail first Run failed to claim all the disks in the drive, only 1 single DG
            - VxRail Log store full
                - I suspect due to the silent, ever present bug, generating logs, the customers log store is already full. 
                - Everytime they are asked to collect logs by support (which has been a lot) a warning pops up and asks the customer to destroy logs to create more
            - PCIe bus errors
                - Describes as cosmetic however errors will pop up in vCenter
                - Errors will not persist
                - Workaround
                    - if Errors continue to flap, check the iDrac to see if it's a real error
            - Other issues persist but are currently seen as related to the above
        - Perception, Customer Experience and Support issues
            - It is important to mention the heavy part of this started around christmas. This made it difficult to always get the right people
            - Product Perception
                - Dollarama does not feel the product is as 'co-engineered and fully tested' as we and our field claim
                - The customer is concerned about all these issues popping up before product with the ultimate question of 'will this be stable in production' 
            - Support Perception
                - Customer believes it took us a long time to solve some critical issues, while at the same time we keep finding more issues almost every day at this point
                - Customer had a perception issues around the performance of the system.
                    - I stepped in here right before  Christmas. I assessed the provided data (HCI Bench) and determined it was fine from a performance standpoint
                    - However after the laundry list of issues and some latency they wanted 'documentation or confirmation' that we had fully tested their specific config
                    - Naveen from EE was called in last week and was able to convince the customer that they did not have a performance issue 
                    - Naveen brought a lot of performance credibility. Would have be good to have him brought in earlier
            - VMware B2B
                - VMware B2B experience was not great this time around
                - Main thing done was filing bugs with response being '7.0 Update 2'
                - We had a scenario were VMware's analysis showed we had a driver not on the HCL. However that is not the case as this was a standard VxRail Image
                    - Further, We had a Sev1 get treated as a Sev 2. Upon trying to debrief, we were told the B2B was not filed correctly and they met their SLO
